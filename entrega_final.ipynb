{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12085873",
   "metadata": {},
   "source": [
    "SE IMPORTAN LAS LIBRERÍAS PERTINENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "599cca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler,MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9f99a",
   "metadata": {},
   "source": [
    "SE CARGAN LOS CSV DE TEST Y TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57bd4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aed560",
   "metadata": {},
   "source": [
    "PASAMOS A NUMERICO Y PONEMOS A NAN TODOS LOS ? Y LOS NÚMEROS NEGATIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01cc7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.apply(pd.to_numeric, errors='coerce')\n",
    "train=train.replace({\"?\":np.nan})\n",
    "train[train < 0] = np.nan\n",
    "\n",
    "test=test.apply(pd.to_numeric, errors='coerce')\n",
    "test=test.replace({\"?\":np.nan})\n",
    "test[test < 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb368b",
   "metadata": {},
   "source": [
    "LAS COLUMNAS DONDE HAY 0 Y ES UN VALOR IRREAL, TAMBIÉN LAS PONEMOS A NULO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0e6eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_error=['trestbps', 'chol','oldpeak']\n",
    "train[cols_error] = train[cols_error].replace({0: np.nan})\n",
    "\n",
    "cols_error=['trestbps', 'chol','oldpeak']\n",
    "test[cols_error] = test[cols_error].replace({0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d810d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2c3af",
   "metadata": {},
   "source": [
    "IMPUTAMOS LOS NULOS CON LA MEDIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52859d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    if train[col].isnull().any():\n",
    "        median_val = train[col].median()\n",
    "        train[col] = train[col].fillna(median_val)\n",
    "\n",
    "    if test[col].isnull().any():\n",
    "        median_val = test[col].median()\n",
    "        test[col] = test[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88cdb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_feature_engineering(df_raw):\n",
    "    \"\"\"\n",
    "    Transforma datos crudos en un perfil de riesgo cardiológico completo.\n",
    "    Nivel Avanzado: Hemodinámica, Metabolismo y Electrofisiología.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- 1. LIMPIEZA INICIAL (Sanity Check) ---\n",
    "    # Convertimos a NaN valores imposibles para que MICE los arregle después\n",
    "    for col in ['trestbps', 'chol']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df[col] = df[col].apply(lambda x: np.nan if x <= 10 else x) # Umbral 10 por seguridad\n",
    "\n",
    "    # --- 2. SUBSISTEMA HEMODINÁMICO (Mecánica de fluidos) ---\n",
    "    \n",
    "    # Rate-Pressure Product (Consumo de O2)\n",
    "    df['RPP'] = df['thalach'] * df['trestbps']\n",
    "    \n",
    "    # Presión Arterial Media Estimada (MAP Proxy)\n",
    "    # Estima la presión de perfusión constante.\n",
    "    df['MAP_Proxy'] = df['trestbps'] * 0.70 \n",
    "\n",
    "    # --- 3. SUBSISTEMA CRONOTRÓPICO (Respuesta del corazón) ---\n",
    "    \n",
    "    # Max Heart Rate Achieved vs Theoretical (Incompetencia Cronotrópica)\n",
    "    # Cuanto más cerca de 1, mejor. Si es bajo, el corazón falla al acelerar.\n",
    "    # Evitamos división por cero en ancianos extremos (aunque 220-age rara vez es 0)\n",
    "    df['HR_Reserve_Ratio'] = df['thalach'] / (220 - df['age'])\n",
    "\n",
    "    # --- 4. SUBSISTEMA METABÓLICO (Química sanguínea) ---\n",
    "    \n",
    "    # Carga Metabólica (Interacción Diabetes-Colesterol)\n",
    "    # Si fbs es 1 (diabético), el colesterol se penaliza el doble.\n",
    "    df['Metabolic_Risk'] = df['chol'] * (1 + df['fbs'])\n",
    "\n",
    "    # --- 5. SUBSISTEMA ELÉCTRICO/ISQUÉMICO (Daño estructural) ---\n",
    "    \n",
    "    # Riesgo Eléctrico (Ya lo teníamos, es muy bueno)\n",
    "    df['ECG_Risk'] = df['oldpeak'] * df['slope']\n",
    "\n",
    "    \n",
    "    # Índice de Angina Combinada\n",
    "    # cp: 1=typical, 2=atypical, 3=non-anginal, 4=asymptomatic\n",
    "    # Transformamos cp para que 1 sea el más grave y 4 el menos (invertimos lógica original si es necesario)\n",
    "    # Asumiendo el dataset original: 4 suele ser asintomático (peligroso en silencioso) o no-cardiaco.\n",
    "    # Vamos a usar la interacción directa Angina Ejercicio + Oldpeak\n",
    "    df['Ischemia_Severity'] = df['exang'] * df['oldpeak']\n",
    "\n",
    "    df['Silent_Ischemia'] = ((df['cp'] == 4) & (df['exang'] == 1)).astype(int)\n",
    "\n",
    "\n",
    "    # --- 6. TRANSFORMACIONES MATEMÁTICAS (Normalización de distribución) ---\n",
    "    # Logaritmos para variables con colas largas (Outliers naturales)\n",
    "    df['log_chol'] = df['chol'].apply(lambda x: np.log1p(x) if x > 0 else np.nan)\n",
    "    df['log_oldpeak'] = df['oldpeak'].apply(lambda x: np.log1p(x) if x >= 0 else np.nan)\n",
    "    \n",
    "    # RPP suele ser un número gigante (ej. 150*120 = 18000), aplicamos log para suavizarlo\n",
    "    df['log_RPP'] = df['RPP'].apply(lambda x: np.log1p(x) if x > 0 else np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236ee9d",
   "metadata": {},
   "source": [
    "AGREGAMOS LAS NUEVAS COLUMNAS DEL FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71ba54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=medical_feature_engineering(train)\n",
    "test=medical_feature_engineering(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9961b",
   "metadata": {},
   "source": [
    "ELIMINAMOS LAS COLUMNAS QUE HEMOS DESCARTADO AL HACER EL EDA TANTO DE LOS NUEVOS DATOS COMO DE LOS VIEJOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21da3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(columns=[\"chol\",\"log_chol\",\"trestbps\",\"log_RPP\",\"log_oldpeak\",\"MAP_Proxy\",\"thal\",\"thalach\"])\n",
    "\n",
    "test=test.drop(columns=[\"chol\",\"log_chol\",\"trestbps\",\"log_RPP\",\"log_oldpeak\",\"MAP_Proxy\",\"thal\",\"thalach\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82c3b9",
   "metadata": {},
   "source": [
    "DIVIDIMOS EN X E y PARA DEFINIR CARACTERÍSTICAS Y ETIQUETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f99f3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"label\"])\n",
    "y = train[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a35c4",
   "metadata": {},
   "source": [
    "PARTIMOS EL SET DE TRAIN PARA PODER HACER LA VALIDACIÓN Y APLICAMOS LA REGRESION LOGISTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33b19743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sevil\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro: 0.48299319727891155\n",
      "F1-macro: 0.292902809906983\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. Train / Validation split\n",
    "# ==========================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # mantener proporción de clases\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 2. Pipeline: scaler + Logistic Regression\n",
    "# ==========================\n",
    "logreg_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        max_iter=400,\n",
    "        multi_class=\"auto\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==========================\n",
    "# 3. Entrenar\n",
    "# ==========================\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# ==========================\n",
    "# 4. Predicción y métricas\n",
    "# ==========================\n",
    "y_pred = logreg_model.predict(X_val)\n",
    "\n",
    "f1_micro = f1_score(y_val, y_pred, average=\"micro\")\n",
    "f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"F1-micro:\", f1_micro)\n",
    "print(\"F1-macro:\", f1_macro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126752b9",
   "metadata": {},
   "source": [
    "PREDECIMOS EL SET DE TEST Y OBTENEMOS LAS PREDICCIONES QUE NOS GUARDAMOS EN UN CSV Y MOSTRAMOS LOS PRIMEROS VALORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45b44897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  label\n",
      "0   0      4\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      2\n",
      "4   4      0\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1️⃣ Preparar test con mismas columnas que X\n",
    "# ==========================\n",
    "X_test = test[X.columns].fillna(0).astype(float)  # rellenar NaN con 0\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ Escalar el test y predecir\n",
    "# ==========================\n",
    "y_test_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ Crear DataFrame con predicciones\n",
    "# ==========================\n",
    "df_predicciones_logreg = pd.DataFrame({\n",
    "    \"ID\":X_test.index,\n",
    "    \"label\": y_test_pred_logreg})\n",
    "\n",
    "print(df_predicciones_logreg.head())\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ Guardar CSV con predicciones\n",
    "# ==========================\n",
    "df_predicciones_logreg.to_csv(\n",
    "    'predicciones.csv',\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
